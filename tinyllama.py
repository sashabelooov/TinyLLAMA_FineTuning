# -*- coding: utf-8 -*-
"""TinyLLAMA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CNX1WdwRJrLXb_jJuXPHk36318LWG3tx
"""

!pip install datasets transformers peft bitsandbytes

import torch
from datasets import load_dataset
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments, Trainer
from peft import LoraConfig, get_peft_model, TaskType

model_name = 'TinyLlama/TinyLlama-1.1B-Chat-v1.0'

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type='nf4',
    bnb_4bit_compute_dtype=torch.float16,
)

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    device_map='auto',
    trust_remote_code=True
)

tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)

lora_config = LoraConfig(
    r = 8,
    lora_alpha = 16,
    target_modules = ['q_proj', 'v_proj'],
    lora_dropout = 0.05,
    bias = 'none',
    task_type = TaskType.CAUSAL_LM
)

model = get_peft_model(model, lora_config)

data = load_dataset('openai/gsm8k', 'main', split='train[:200]')

def tokenize(batch):
  texts = [
      f'### Instruction: \n{instruction}\n### Response:\n{output}'
      for instruction, output in zip(batch['question'], batch['answer'])
      ]

  tokens = tokenizer(
      texts,
      padding = 'max_length',
      max_length = 256,
      truncation = True,
      return_tensors = 'pt'
  )

  tokens['labels'] = tokens['input_ids'].clone()

  return tokens

tokenized_data = data.map(tokenize, batched=True, remove_columns=data.column_names)

training_args = TrainingArguments(
  output_dir = './tinyllama-math-lora-tutorial',
  per_device_train_batch_size = 4,
  gradient_accumulation_steps = 4,
  learning_rate = 1e-3,
  num_train_epochs = 50,
  fp16 = True,
  logging_steps = 20,
  save_strategy = 'epoch',
  report_to = 'none',
  remove_unused_columns = False,
  label_names = ['labels']
 )

trainer = Trainer(
    model = model,
    args = training_args,
    train_dataset = tokenized_data,
    processing_class = tokenizer
)

trainer.train()

model.save_pretrained('./tinyllama-lora-tuned-adapter-math')
tokenizer.save_pretrained('./tinyllama-lora-tuned-adapter-math')



import os
import math

import torch
from torch.utils.data import DataLoader

from datasets import load_dataset
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, default_data_collator

from peft import PeftModel

model_name = 'TinyLLama/TinyLlama-1.1B-Chat-v1.0'
adapter_path = './tinyllama-lora-tuned-adapter-math'

bnb_config = BitsAndBytesConfig(
    load_in_4bit = True,
    bnb_4bit_quant_type = 'nf4',
    bnb_4bit_compute_dtype = torch.bfloat16
)

tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)

base_model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config = bnb_config,
    device_map = 'auto',
    trust_remote_code = True
).eval()

tmp_model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config = bnb_config,
    device_map = 'auto',
    trust_remote_code = True
)

tuned_model = PeftModel.from_pretrained(tmp_model, adapter_path)
tuned_model = tuned_model.merge_and_unload().eval()

def tokenize(batch):
    texts = [
        f"### Instruction:\n{inst}\n### Response:\n{out}"
        for inst, out in zip(batch['question'], batch['answer'])
    ]

    tokens = tokenizer(
        texts,
        padding = 'max_length',
        truncation = True,
        max_length = 256,
        return_tensors = 'pt'
    )

    tokens['labels'] = tokens['input_ids'].clone()

    return tokens

eval_ds = load_dataset('openai/gsm8k', 'main', split='train[:20]')
eval_ds = eval_ds.map(tokenize, batched=True, remove_columns=['question', 'answer'])
eval_ds = eval_ds.with_format('torch')

eval_loader = DataLoader(
    eval_ds,
    batch_size = 8,
    collate_fn = default_data_collator
)

@torch.no_grad()
def compute_perplexity(model):
    losses = []

    for batch in eval_loader:
        batch = {k: v.to('cuda') for k, v in batch.items()}
        loss = model(**batch).loss
        losses.append(loss.item())

    return math.exp(sum(losses) / len(losses))

print(f'Base Model Perplexity: {compute_perplexity(base_model):.2f}')
print(f'Tuned Model Perplexity: {compute_perplexity(tuned_model):.2f}')

import random

raw_data = load_dataset('gsm8k', 'main', split='train[:20]')
refs = raw_data['answer']


def generate(model, instruction):
    token_ids = tokenizer(f'### Instruction:\n{instruction}\n### Response:\n', return_tensors='pt').input_ids.to('cuda')

    with torch.no_grad():
        out = model.generate(token_ids, max_new_tokens=256)

    #return tokenizer.decode(out[0], skip_special_tokens=True).split('### Response:\n')[-1].strip()
    return tokenizer.decode(out[0], skip_special_tokens=True)

raw_data['question'][1]

print(generate(base_model, raw_data['question'][1]))

print(generate(tuned_model, raw_data['question'][1]))

print(refs[1])

eval_ds = load_dataset('openai/gsm8k', 'main', split='train[200:300]')
eval_ds = eval_ds.map(tokenize, batched=True, remove_columns=['question', 'answer'])
eval_ds = eval_ds.with_format('torch')

eval_loader = DataLoader(
    eval_ds,
    batch_size = 8,
    collate_fn = default_data_collator
)

print(f'Base Model Perplexity: {compute_perplexity(base_model):.2f}')
print(f'Tuned Model Perplexity: {compute_perplexity(tuned_model):.2f}')

raw_data = load_dataset('gsm8k', 'main', split='train[200:300]')
refs = raw_data['answer']


def generate(model, instruction):
    token_ids = tokenizer(f'### Instruction:\n{instruction}\n### Response:\n', return_tensors='pt').input_ids.to('cuda')

    with torch.no_grad():
        out = model.generate(token_ids, max_new_tokens=256)

    #return tokenizer.decode(out[0], skip_special_tokens=True).split('### Response:\n')[-1].strip()
    return tokenizer.decode(out[0], skip_special_tokens=True)

raw_data['question'][0]

print(generate(base_model, raw_data['question'][0]))

print(generate(tuned_model, raw_data['question'][0]))

print(refs[0])



!pip install fastapi uvicorn
!pip install pyngrok

# Commented out IPython magic to ensure Python compatibility.
# %%writefile main.py
# from fastapi import FastAPI
# from pydantic import BaseModel
# import torch
# from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
# from peft import PeftModel
# from fastapi.middleware.cors import CORSMiddleware
# 
# app = FastAPI()
# 
# class Query(BaseModel):
#     text: str
# 
# 
# # --- ADD THIS SECTION ---
# app.add_middleware(
#     CORSMiddleware,
#     allow_origins=["*"],  # Allows all origins
#     allow_credentials=True,
#     allow_methods=["*"],  # Allows all methods
#     allow_headers=["*"],  # Allows all headers
# )
# # ------------------------
# 
# 
# # Load model once at startup
# model_name = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
# adapter_path = "./tinyllama-lora-tuned-adapter-math"
# 
# bnb_config = BitsAndBytesConfig(
#     load_in_4bit=True,
#     bnb_4bit_quant_type="nf4",
#     bnb_4bit_compute_dtype=torch.bfloat16,
# )
# 
# base_model = AutoModelForCausalLM.from_pretrained(
#     model_name, quantization_config=bnb_config, device_map="auto", trust_remote_code=True
# )
# model = PeftModel.from_pretrained(base_model, adapter_path)
# model = model.merge_and_unload()
# model.eval()
# 
# tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
# 
# @app.post("/generate")
# async def generate(query: Query):
#     prompt = f"### Instruction:\n{query.text}\n### Response:\n"
#     inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
# 
#     with torch.no_grad():
#         output = model.generate(**inputs, max_new_tokens=256, do_sample=True, temperature=0.7)
# 
#     response = tokenizer.decode(output[0], skip_special_tokens=True)
#     response = response.split("### Response:\n")[-1].strip()
# 
#     return {"response": response}

from pyngrok import ngrok
from google.colab import userdata

# Kill any process on port 8000
!fuser -k 8000/tcp

# Kill all ngrok tunnels
!pkill -f ngrok

# Get the ngrok authentication token from Colab secrets
NGROK_AUTHTOKEN = userdata.get('NGROK_AUTHTOKEN')
ngrok.set_auth_token(NGROK_AUTHTOKEN)

public_url = ngrok.connect(8000)
public_url

!uvicorn main:app --host 0.0.0.0 --port 8000

# Kill any process on port 8000
!fuser -k 8000/tcp

# Kill all ngrok tunnels
!pkill -f ngrok

